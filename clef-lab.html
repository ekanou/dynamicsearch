---
layout: clef-default
---

<!-- Banner -->
<section id="banner">
  	<div class="inner">
    		<h2>CLEF 2017 Lab</h2>
    		<p>Dynamic Search for Complex Tasks</p>
  	</div>
</section>

<!-- Main -->
<section id="Lab Overview" class="wrapper style1">
        <div class="container">
            	<div class="row">
			<section>
            			<h3>Background</h3>
                		<p>Information Retrieval research has traditionally focused on serving the best results for a single query — so-called ad hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. A key challenge in the study of this interaction is the creation of suitable evaluation resources to assess the effectiveness of IR systems over sessions.</p>
				<p> The TREC Interactive track one of the first track’s at TREC, with the goal of investigating interactive information retrieval by examining the process as well as the results. It ran for nine years, from 1994 to 2002, and led to a variety of findings: Presentation of documents matters to users, i.e., better surrogates help but clustering does not; users do not utilize relevance feedback as we might think/hope; results from “batch” studies do not necessarily apply to real-world searchers. Further, it resulted in an early standardization of the experimental design towards evaluating interactive information retrieval systems. However, the track did not lead to a reusable test collection methodology.</p>
				<p> The High Accuracy Retrieval of Documents (HARD) track followed the Interactive track, with the primary focus on single-cycle user-system interactions. These interactions were embodied in clarification forms which could be used by retrieval algorithms to elicit feedback from assessors. The track attempted to further standardize the retrieval of interactive algorithms, however it also did not lead to a reusable collection, in the sense that new retrieval algorithms might not have received the same clarification forms as those participating in the experiment, had they interacted with the searchers.</p>
				<p> The TREC Session Track, which ran from 2010 through 2014, had always had two primary goals: (1) to test whether systems can improve their performance for a given query by using previous user interactions with a retrieval system (including previous queries, clicks on ranked results, dwell times, etc.), and (2) to evaluate system performance over an entire query session instead of a single query. However, it only produced test collections to evaluate  the first goal, given that the second one required a dynamic test collection, where users react upon the results returned by a retrieval system.</p>
				<p>In 2015 the TREC Tasks track was introduced to test whether systems upon a user’s query could understand the underlying information access task and provide documents for all possible subtasks as a way to evaluate the second goal of the Session track. However, this stripped out the task from any user interactions and the dynamics of any such collection.</p>
			</section>
		</div>
	</div>
</section>

<section id="Objectives" class="wrapper style2">
        <div class="container">
            	<div class="row">
			<section>
				<h3>Lab Overview</h3>
				<p>The present lab attempts to bridge the batch TREC-style evaluation methodology and the Interactive Information Retrieval evaluation methodology, with the goal of evaluating system performance over an <strong>entire session</strong>, keeping the “user” in the loop, but at the same time leading to reusable test collections.</p>
				
				<h3>Objectives</h3>
		    		<p>The <b>objective</b> of the lab is threefold:</p>
                    		<ol>
                        	<li>to produce the methodology and algorithms that will lead to a <em>dynamic test collection</em> by simulating the users</li>
                        	<li>to understand and quantify in terms of evaluation measures what constitutes a good ranking of documents at <em>different stages of a session</em>, and a good ranking for the overall session</li>
                        	<li>to develop algorithms that can provide an optimal ranking <em>throughout a user's session</em>.</li>
                    		</ol>
			</section>
		</div>
	</div>
</section>

<section id="Papers" class="wrapper style1">
     	<div class="container">
            	<div class="row">
			<section>
		<h3>Paper Submission</h3>
                <p>The focus of this year's workshop will be the evaluation of interactive information retrieval algorithms. We solicit the submission of two types of papers: (a) position papers, and (b) data papers.</p>
					<ol>
						<li><em>Position papers</em> focusing on evaluation methodologies for assessing the quality of search algorithms with the user in the loop, under two constraints: (1) any evaluation framework proposed should allow the (statistical) reproducibility of results, and (2) it should lead to a reusable benchmark collection.</li>
						<li><em>Data Papers</em> describing test collections or data sets suitable for guiding the construction of dynamic test collections, tasks and evaluation metrics.</li>
					</ol>
			</section>
		</div>
	</div>
</section>

<section id="Pilot Task" class="wrapper style2">
     	<div class="container">
            	<div class="row">
			<section>
			<h3>Pilot Task</h3>
			<p>Task Completion Engines and Autonomous Search Agents are now being developed to help users in acquiring information in order to make a decision and complete a task. At the same time such an Autonomous Search Agent could be used in simulating user queries for the evaluation of dynamic search algorithms.</p>

			<p>Such engines/agents need to work with a user to ascertain their information needs, then perform their own searches to dynamically identify relevant material, which will be useful in completing a particular task. For example, consider the task of organising a wedding. There are many different things that that need to be arranged and ordered, e.g. a venue, flowers, catering, gift list, dresses, car hire, hotels, etc. Finding relevant sites and resources requires numerous searches and filtering through many documents/sites. A search agent could help to expedite the process by finding the relevant sites to visit, while a task completion engine would provide a structured interface to help complete the process.</p>

			<p>In this year's Dynamic Search Task Track, the task is to generate a series of queries that a search agent would issue to a search engine, in order to compile a set of links useful for completing the task at hand. This set might be presented to the user or used for further processing by a task completion engine.</p>

			<p>As a starting point, we will consider how humans source information required to complete such tasks, and use this information as a reference point for our evaluation, i.e. can we develop search agents that can generate queries that are like those posed by humans? Thus the focus of the track will be on query generation and models of querying.</p>
			</section>
			
			<!--		
                    <dl>
			<dt> Dataset: ClueWeb12B (An API to call a pre-defined retrieval algorithm will be provided to the participants) 
                                    	<dt> Input: </dt>
                                    	<dt>
                                    	<ol>
                                    	<li>Task description</li>
                                    	<li>The first query of a user in a session</li>
                                    	</ol>
                                    	</dt>
                                    	<dt>Output: Ranking of the provided set of queries.</dt>
                                    	<dt>Evaluation Measure: Precision@1</dt>
                                  	</dl>
-->
		</div>
	</div>
</section>
        

<section id="Important Dates" class="wrapper style1">            
        <div class="container">
            	<div class="row">
                    	<h3>Important Dates</h3>
                    	<ul>
                        	<li>Labs registration opens: 4 November 2016</li>
				<li>Pilot task data release: 17 March 2017</li>
                        	<li>Registration closes: 21 April 2017</li>
				<li>End of the Evaluation Cycle: 5 May 2017</li>
                        	<li>Submission of Participant Papers [CEUR-WS]: 26 May 2017</li>
                        	<li>Notification of Acceptance Participant Papers [CEUR-WS]: 16 June 2017</li>
                        	<li>Camera Ready Copy of Participant Papers [CEUR-WS]: 3 July 2017</li>
                    	</ul>
            	</div>
        </div>
</section>
