---
layout: clef-default
---

<!-- Main -->
<section id="main" class="wrapper style1">

        <!-- Banner -->
        <section id="complexinteraction">
            <div class="inner">
                <h2>CLEF 2017 La</h2>
                <p>Dynamic Search for Complex Tasks</p>
                <ul class="actions">
                    <li><a href="" class="button big special">CLEF 2017 Registration</a></li>
                </ul>
            </div>
        </section>

        <div class="container">
            <div class="row">
                <section id="Lab Overview">
                    <h2>Lab Overview</h2>
                    <p>The <strong>objective</strong> of the lab is threefold:<\p>
                    <ol>
                        <li>to produce the methodology and algorithms that will lead to a <em>dynamic test collection</em> by simulating the users</li>
                        <li>to understand and quantify in terms of evaluation measures what constitutes a good ranking of documents at <em>different stages of a session</em>, and a good overall session</li>
                        <li>to lead to algorithms that can provide an optimal ranking throughout a session.</li>
                    </ol>
                </section>
            </div>
        </div>
        
        <div class="container">
            <div class="row">
                <section id="Proposed Tasks">
                    <h3>Proposed Tasks</h3>
                        <ol>
                            <dt>Task 1:</dt>
                            <dd>
                            Construction of dynamic collections (simulations). The first task will evaluate simulations of user interactions. Simulations can be evaluated against static user logs, or actual users. This is a topic of discussion during the proposed workshop.
                            </dd>
                            <dt>Task 2:</dt>
                            <dd>Session search evaluation. The second task will evaluate proposed metrics both for overall session evaluation, and for step足by足step session evaluation. Metrics should be applied on a reusable test collection, however they can be (meta足) evaluated against an interactive experiment with actual users.
                            </dd>
                            <dt> Task 3:</dt>
                            <dd>Session search. The third task will evaluate retrieval effectiveness on the results of the two previous tasks. This third task can run in a follow足up year of the lab, after Task 1 and Task 2 have established the details of the evaluation framework.
                            </dd>
                        </ol>
                </section>
            </div>
        </div>
        
        <div class="container">
            <div class="row">        
                <section id="Paper Submission">
                    <h3>Paper Submission</h3>
                    <p> We solicit the submission of position papers over the following topics.
                    </p>
                </section>
            </div>
        </div>
        
        <div class="container">
            <div class="row">
                <section id="Important Dates">
                    <h3>Important Dates</h3>
                    <ul>
                        <li>Labs registration opens: 4 November 2016</li>
                        <li>Registration closes: 21 April 2017</li>
                        <li>Submission of Participant Papers [CEUR-WS]: 26 May 2017</li>
                        <li>Notification of Acceptance Participant Papers [CEUR-WS]: 16 June 2017</li>
                        <li>Camera Ready Copy of Participant Papers and Extended Lab Overviews [CEUR-WS] due: 3 July 2017</li>
                    </ul>
                </section>
            </div>
        </div>
    </div>
</section>
