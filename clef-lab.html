---
layout: clef-default
---

<!-- Banner -->
<section id="banner">
  	<div class="inner">
    		<h2>CLEF 2017 Lab</h2>
    		<p>Dynamic Search for Complex Tasks</p>
  	</div>
</section>

<!-- Main -->
<section id="Lab Overview" class="wrapper style1">
        <div class="container">
            	<div class="row">
			<section>
            			<h3>Lab Overview</h3>
                		<p>Information Retrieval research has traditionally focused on serving the best results for a single query — so-called ad hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. A key challenge in the study of this interaction is the creation of suitable evaluation resources to assess the effectiveness of IR systems over sessions. The TREC Session Track, which ran from 2010 through to 2014, had always had two primary goals: (1) to test whether systems can improve their performance for a given query by using previous user interactions with a retrieval system (including previous queries, clicks on ranked results, dwell times, etc.), and (2) to evaluate system performance over an entire query session instead of a single query. However, it only produced test collections to evaluate  the first goal, given that the second one required a dynamic test collection, where users react upon the results returned by a retrieval system. In 2015 the TREC Tasks track was introduced to test whether systems upon a user’s query could understand the underlying information access task and provide documents for all possible subtasks as a way to evaluate the second goal of the Session track. However, this stripped out the task from any user interactions and the dynamics of any such collection. The present lab attempts to bridge the two evaluation exercises, with the goal of evaluating system performance over an entire session, keeping the “user” in the loop.</p>
			</section>
		</div>
	</div>
</section>

<section id="Objectives" class="wrapper style2">
        <div class="container">
            	<div class="row">
			<section>
				<h3>Objectives</h3>
		    		<p>The <b>objective</b> of the lab is threefold:</p>
                    		<ol>
                        	<li>to produce the methodology and algorithms that will lead to a <em>dynamic test collection</em> by simulating the users</li>
                        	<li>to understand and quantify in terms of evaluation measures what constitutes a good ranking of documents at <em>different stages of a session</em>, and a good ranking for the overall session</li>
                        	<li>to develop algorithms that can provide an optimal ranking <em>throughout a user's session</em>.</li>
                    		</ol>
			</section>
		</div>
	</div>
</section>

<section id="Participation" class="wrapper style1">
     	<div class="container">
            	<div class="row">
			<section>
		 	<h3>Participation</h3>
                        <p>This is the first year that this workshop runs and there are two possible ways to participate:</p>
                        <div id="Submissions">
                                <ul class="tab">
                                        <li><a href="javascript:void(0)" class="tablinks" onclick="openTab(event, 'Papers')" id="defaultOpen">Scientific Papers</a></li>
                                        <li><a href="javascript:void(0)" class="tablinks" onclick="openTab(event, 'Pilot Task')">Pilot Task</a></li>
                                </ul>

                                <div id="Papers" class="tabcontent">
                                        <p>The focus of this year's workshop will be the evaluation of interactive information retrieval algorithms. We solicit the submission of two types of papers: (a) position papers, and (b) data papers.</p>
					<ol>
						<li><em>Position papers</em> focusing on evaluation methodologies for assessing the quality of search algorithms with the user in the loop, under two constraints: (1) any evaluation framework proposed should allow the (statistical) reproducibility of results, and (2) it should lead to a reusable benchmark collection.</li>
						<li><em>Data Papers</em> describing test collections or data sets suitable for guiding the construction of dynamic test collections, tasks and evaluation metrics.</li>
					</ol>
				</div>

                                <div id="Pilot Task" class="tabcontent">
                                  	<p>The focus of this year's pilot task will be the simulation of user's query reformulations. This is the first and hardest step towards constructing dynamic test collections. Ideally one would like to be able to generate query reformulations. However, for the purpose of this workshop the pilot task will focus on predicting which user's query comes next out of a set of user queries on a given topic.</p>
                                  	<dl>
					<dt> Dataset: The dataset to be used will be the TREC 2014 Session Track.
                                    	<dt> Input: </dt>
                                    	<dt>
                                    	<ol>
                                    	<li>Task description</li>
                                    	<li>The first query of a user in a session</li>
                                    	<li>The document ranking for the first query, the clicks, the dwell times, and the relevance labels</li>
                                    	<li>A set of possible queries</li>
                                    	</ol>
                                    	</dt>
                                    	<dt>Output: Ranking of the provided set of queries.</dt>
                                    	<dt>Evaluation Measure: Precision@1</dt>
                                  	</dl>
                                </div>
                        </div>
			</section>
            	</div>
	</div>
</section>
	
        <!--
        <div class="container">
            <div class="row">
                <section id="Proposed Tasks">
                    <h3>Proposed Tasks</h3>
                        <ol>
                            <dt>Task 1:</dt>
                            <dd>
                            Construction of dynamic collections (simulations). The first task will evaluate simulations of user interactions. Simulations can be evaluated against static user logs, or actual users. This is a topic of discussion during the proposed workshop.
                            </dd>
                            <dt>Task 2:</dt>
                            <dd>Session search evaluation. The second task will evaluate proposed metrics both for overall session evaluation, and for step­by­step session evaluation. Metrics should be applied on a reusable test collection, however they can be (meta­) evaluated against an interactive experiment with actual users.
                            </dd>
                            <dt> Task 3:</dt>
                            <dd>Session search. The third task will evaluate retrieval effectiveness on the results of the two previous tasks. This third task can run in a follow­up year of the lab, after Task 1 and Task 2 have established the details of the evaluation framework.
                            </dd>
                        </ol>
                </section>
            </div>
        </div>
        -->

<section id="Important Dates" class="wrapper style2">            
        <div class="container">
            	<div class="row">
                    	<h3>Important Dates</h3>
                    	<ul>
                        	<li>Labs registration opens: 4 November 2016</li>
				<li>Pilot task data release: Beginning of December</li>
                        	<li>Registration closes: 21 April 2017</li>
				<li>End of the Evaluation Cycle: 5 May 2017</li>
                        	<li>Submission of Participant Papers [CEUR-WS]: 26 May 2017</li>
                        	<li>Notification of Acceptance Participant Papers [CEUR-WS]: 16 June 2017</li>
                        	<li>Camera Ready Copy of Participant Papers [CEUR-WS]: 3 July 2017</li>
                    	</ul>
            	</div>
        </div>
</section>

<section id="Discussion Forum" class="wrapper style1">            
        <div class="container">
            	<div class="row">
                    	<h3>Forum</h3>
<iframe id="forum_embed"
 src="javascript:void(0)"
 scrolling="no"
 frameborder="0"
 width="900"
 height="700">
</iframe>

<script type="text/javascript">
 document.getElementById("forum_embed").src =
  "https://groups.google.com/forum/embed/?place=forum/clef-dynamic-search" +
  "&showsearch=true&showpopout=true&parenturl=" +
  encodeURIComponent(window.location.href);
</script>
		</div>
	</div>
</section>
