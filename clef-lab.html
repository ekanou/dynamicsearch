---
layout: clef-default
---

<!-- Banner -->
<section id="banner">
  <div class="inner">
    <h2>CLEF 2017 Lab</h2>
      <p>Dynamic Search for Complex Tasks</p>
  </div>
</section>

<!-- Main -->
<section id="main" class="wrapper style1">
        <div class="container">
            <div class="row">
                <section id="Lab Overview">
                    <h3>Lab Overview</h3>
                    <p> Information Retrieval research has traditionally focused on serving the best results for a single query — so-called ad hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. A key challenge in the study of this interaction is the creation of suitable evaluation resources to assess the effectiveness of IR systems over sessions. The TREC Session Track, which ran from 2010 through to 2014, had always had two primary goals: (1) to test whether systems can improve their performance for a given query by using previous user interactions with a retrieval system (including previous queries, clicks on ranked results, dwell times, etc.), and (2) to evaluate system performance over an entire query session instead of a single query. However, it only produced test collections to evaluate  the first goal, given that the second one required a dynamic test collection, where users react upon the results returned by a retrieval system. In 2015 the TREC Tasks track was introduced to test whether systems upon a user’s query could understand the underlying information access task and provide documents for all possible subtasks as a way to evaluate the second goal of the Session track. However, this stripped out the task from any user interactions and the dynamics of any such collection. The present lab attempts to bridge the two evaluation exercises, with the goal of evaluating system performance over an entire session, keeping the “user” in the loop.</p>
                    <p>The <strong>objective</strong> of the lab is threefold:</p>
                    <ol>
                        <li>to produce the methodology and algorithms that will lead to a <em>dynamic test collection</em> by simulating the users</li>
                        <li>to understand and quantify in terms of evaluation measures what constitutes a good ranking of documents at <em>different stages of a session</em>, and a good ranking for the overall session</li>
                        <li>to develop algorithms that can provide an optimal ranking <em>throughout a user's session</em>.</li>
                    </ol>
                </section>
            </div>
        </div>
</section>

<section id="participate" class="wrapper style2">
     <div class="container">
            <div class="row">        
                <section id="Participation">
                        <h3>Participation</h3>
                        <p>This is the first year that this workshop runs and there are two possible ways to participate:</p>
                        <div id="Submissions">
                                <ul class="tab">
                                        <li><a href="javascript:void(0)" class="tablinks" onclick="openTab(event, 'Papers')" id="defaultOpen">Scientific Papers</a></li>
                                        <li><a href="javascript:void(0)" class="tablinks" onclick="openTab(event, 'Pilot Task')">Pilot Task</a></li>
                                </ul>

                                <div id="Papers" class="tabcontent">
                                        <p>We solicit the submission of two types of papers: position papers, and papers describing data collection or living lab opportunities.</p>
                                        <p><em>Position papers</em> focusing on evaluation methodologies for assessing the quality of search algorithms with the user in the loop, under two constraints: (1) any evaluation framework proposed should allow the (statistical) reproducibility of results, and (2) it should lead to a reusable benchmark collection.</p>
                                        <p><em>Data Papers</em> describing test collections or data sets suitable for guiding the construction of dynamic test collections, tasks and evaluation metrics.</p>
                                        <p> </div>

                                <div id="Pilot Task" class="tabcontent">
                                  <p>Submit a run that simulates query reformulations</p>
                                  <p> Dataset: TREC 2014 Session Track
                                  <dl>
                                    <dt> Input: </dt>
                                    <dt>
                                    <ol>
                                    <li>Task description</li>
                                    <li>The first query of a user in a session</li>
                                    <li>The ranking for the first query, the clicks, the dwell times, and the relevance labels</li>
                                    <li>The set of queries for the same  but from all of the users that tried to complete this task, throughout their sessions</li>
                                    </ol>
                                    </dt>
                                    <dt>Output: Ranking of next user's query.</dt>
                                    <dt>Evaluation Measure: # of user queries correctly predicted</dt>
                                  </dl>
                                </div>
                        </div>
                </section>
            </div>
        </div>
</section>

<section id="participate" class="wrapper style2">
	<div class="container">
			<div class="row">
				<div class="col-lg-8 col-lg-offset-2 text-center">
					<h2 class="section-heading">Participate</h2>
					<hr class="dark">
					<p class="text-muted"
						style="text-align: justify; text-justify: inter-word;">This is
						the first year that this workshop runs and there are two possible
						ways to participate:</p>
					<ul class="nav nav-tabs">
						<li class="active"><a data-toggle="tab" href="#papers">Research
								Papers</a></li>
						<li><a data-toggle="tab" href="#task">Pilot Task</a></li>

					</ul>
            				<div class="tab-content">
						<div id="papers" class="tab-pane fade in active">
							<h3>Research Papers Submission</h3>
							<p style="text-align: justify; text-justify: inter-word;">
								The workshop is <b>open to the submission of papers</b>
								describing test collections or data sets suitable for early risk
								prediction, early risk prediction challenges, tasks and
								evaluation metrics or specific early risk detection solutions.
								We understand that there are two main classes of early risk
								prediction:
							</p>
							<ul class="list-group">
								<li class="list-group-item"
									style="text-align: justify; text-justify: inter-word;"><b>With
										multiple actors</b>. We include in this category cases where there
									is an external actor or intervening factor that explicitly
									causes or stimulates the problem. For instance, sexual
									offenders use deliberate tactics to contact vulnerable children
									and engage them in sexual abuse. In such cases, early warning
									systems need to analyse the interactions between the offender
									and the victim and, in particular, the language of both.
									Another example of risk provoked by external actions is
									terrorist recruitment; there is currently massive online
									activity aiming at recruiting young people (particularly,
									teenagers) for joining criminal networks</li>
								<li class="list-group-item"
									style="text-align: justify; text-justify: inter-word;"><b>With
										a single actors</b>. We include in this second category cases
									where there is no explicit external actor or intervening factor
									that causes or stimulates the problem, but instead the risk
									comes “exclusively” from the individual. For instance, a
									teenager developing depression whose process is not caused or
									stimulated by any intervention or action made by another
									individual. Of course, there might be multiple personal or
									contextual factors that affect (or even cause) this process
									and, as a matter of fact, this is usually the case. However, as
									it is notfeasible to have access to sources of data associated
									to all these external conditions, the only element that can be
									analysed is the language of the individual.</li>
							</ul>
							<p style="text-align: justify; text-justify: inter-word;">The
								two classes of risks described above might interact one to each
								other. For instance, individuals suffering from major depression
								might be more inclined to fall prey to criminal networks. From a
								technological perspective, different types of tools are likely
								needed to develop early warning systems for these two types of
								risks.</p>
							<p style="text-align: justify; text-justify: inter-word;">Essentially,
								we look at early risk prediction as a process of sequential
								evidence accumulation where alerts are made when there is enough
								evidence about a certain type of risk. For the single actor type
								of risk, the pieces of evidence could come from the
								chronological sequence of entries written by a tormented subject
								in the Social Media. For the multiple actor type of risk, the
								pieces of evidence could come from a series of messages
								interchanged by an offender and a victim in a chatroom or online
								forum.</p>
							<p style="text-align: justify; text-justify: inter-word;">Notice
								that we refer to early risk in a general way and the workshop is
								open to contributions along these lines in any possible
								application domain.</p>
							<a href="#dates" class="btn btn-primary btn-xl page-scroll">Important
								Dates</a>
						</div>
						<div id="task" class="tab-pane fade">
							<h3>Early Detection of Depression</h3>
							<p style="text-align: justify; text-justify: inter-word;">
								The second way of participation consists in performing a <b>pilot
									task on early risk detection of depression</b>. This is an
								exploratory task on early risk detection of depression. The
								challenge consists of sequentially processing pieces of evidence
								and detect early traces of depression as soon as possible. The
								task is mainly concerned about evaluating Text Mining solutions
								and, thus, it concentrates on texts written in Social Media.
								Texts should be processed in the order they were created. In
								this way, systems that effectively perform this task could be
								applied to sequentially monitor user interactions in blogs,
								social networks, or other types of online media.
							</p>
							<p style="text-align: justify; text-justify: inter-word;">
								The test collection for this pilot task is the collection
								described in <a
									href="https://citius.usc.es/investigacion/publicacions/listado/a-test-collection-research-depression-language-use">[Losada
									&amp; Crestani 2016]</a>. It is a collection of writings (posts or
								comments) from a set of Social Media users. There are two
								categories of users, depressed and non- depressed, and, for each
								user, the collection contains a sequence of writings (in
								chronological order). For each user, his collection of writings
								has been divided into 10 chunks. The first chunk contains the
								oldest 10% of the messages, the second chunk contains the second
								oldest 10%, and so forth.
							</p>
							<p style="text-align: justify; text-justify: inter-word;">The
								task is organized into two different stages:</p>
							<ul class="list-group">

								<li class="list-group-item"
									style="text-align: justify; text-justify: inter-word;"><b>Training
										stage</b>. Initially, the teams that participate in this task will
									have access to a training stage where we will release the whole
									history of writings for a set of training users (we will
									provide all chunks of all training users), and we will indicate
									what users have explicitly mentioned that they have been
									diagnosed with depression. The participants can therefore tune
									their systems with the training data.</li>
								<li class="list-group-item"
									style="text-align: justify; text-justify: inter-word;"><b>Test
										stage.</b> The test stage will consist of 10 sequential releases
									of data (i.e. done at different dates). The first release will
									consist of the 1st chunk of data (oldest writings of all test
									users), the second release will consist of the 2nd chunk of
									data (second oldest writings of all test users), and so forth.
									After each release, the participants will have a few days to
									process the data and, before the next release, each
									participating system has to choose between two options: a)
									emitting a decision on the user (i.e. depressed or
									non-depressed), or b) seeing more chunks. 
									This choice has to be made for each user in the collection. If the system emits a
									decision, its decision will be final and it will be evaluated
									based on the correctness of the system's decision and the
									number of chunks required to make the decision (using a metric
									for which the fewer writings required to make the alert, the
									better). If the system does not emit a decision then it will
									have access to the next chunk of data, that is it will have
									access to more writings, but it will have a penalty for a
									“later emission”).</li>
							</ul>
							<p style="text-align: justify; text-justify: inter-word;">
								Evaluation: The evaluation will take into account not only the
								correctness of the system's output (i.e. whether or not the user
								is depressed) but also the delay taken to emit its decision. To
								meet this aim, we will consider the ERDE metric proposed in <a
									href="https://citius.usc.es/investigacion/publicacions/listado/a-test-collection-research-depression-language-use">[Losada
									&amp; Crestani 2016]</a>.
							</p>
							
							<!-- <p style="text-align: justify; text-justify: inter-word;">
								The participants can use external sources of data to tune their algorithms. 
								However, the participating systems cannot resort to data obtained from Reddit to enrich their estimations. 
							</p>
							-->
							<p style="text-align: justify; text-justify: inter-word;">Being
								a pilot task, we expect it to be useful for instigating
								discussion on how to create evaluation laboratories for early
								risk prediction: proper size of the data, adequate early risk
								evaluation metrics, alternative ways to formulate early
								detection tasks, other possible application domains, etc.</p>
							<a href="https://docs.google.com/forms/d/e/1FAIpQLSemmyjxF6M2UUAalWDcwOVbnvGxGEBdyo81jdhAXsabEudYWg/viewform?c=0&w=1" class="btn btn-primary btn-xl page-scroll">Pre-register</a>
						</div>
					</div>
				</div>
		</div>
	</div>
</section>
				
        <!--
        <div class="container">
            <div class="row">
                <section id="Proposed Tasks">
                    <h3>Proposed Tasks</h3>
                        <ol>
                            <dt>Task 1:</dt>
                            <dd>
                            Construction of dynamic collections (simulations). The first task will evaluate simulations of user interactions. Simulations can be evaluated against static user logs, or actual users. This is a topic of discussion during the proposed workshop.
                            </dd>
                            <dt>Task 2:</dt>
                            <dd>Session search evaluation. The second task will evaluate proposed metrics both for overall session evaluation, and for step­by­step session evaluation. Metrics should be applied on a reusable test collection, however they can be (meta­) evaluated against an interactive experiment with actual users.
                            </dd>
                            <dt> Task 3:</dt>
                            <dd>Session search. The third task will evaluate retrieval effectiveness on the results of the two previous tasks. This third task can run in a follow­up year of the lab, after Task 1 and Task 2 have established the details of the evaluation framework.
                            </dd>
                        </ol>
                </section>
            </div>
        </div>
        -->
        
<section id="participate" class="wrapper style3">            
        <div class="container">
            <div class="row">
                <section id="Important Dates">
                    <h3>Important Dates</h3>
                    <ul>
                        <li>Labs registration opens: 4 November 2016</li>
                        <li>Registration closes: 21 April 2017</li>
                        <li>Submission of Participant Papers [CEUR-WS]: 26 May 2017</li>
                        <li>Notification of Acceptance Participant Papers [CEUR-WS]: 16 June 2017</li>
                        <li>Camera Ready Copy of Participant Papers and Extended Lab Overviews [CEUR-WS] due: 3 July 2017</li>
                    </ul>
                </section>
            </div>
        </div>
</section>
