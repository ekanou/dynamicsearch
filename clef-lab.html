---
layout: clef-default
---

<!-- Banner -->
<section id="banner">
  <div class="inner">
    <h2>CLEF 2018 Lab</h2>
    <p>Dynamic Search for Complex Tasks</p>
  </div>
</section>

<!-- Main -->

<section id="Timeline" class="wrapper style1">            
  
    <div class="row">
      <h3>Timeline</h3>
      <p>
	<ul>
	  <li>Registration deadline: 27 April 2018</li>
	  <li>Training data: 15 April 2018</li>
	  <li>Testing data: 5 May 2018</li>
	  <li>Submission deadline: 11 May 2018</li>
	</ul>
      </p>
    </div>
  
</section>

<section id="Background" class="wrapper style2">
  <div class="container">
    <div class="row">
      <section>
        <h3>Background</h3>
        <p>
	  Information Retrieval (IR) research has traditionally focused on serving the best results for a single query — so-called ad hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. A key challenge in the study of this interaction is the creation of suitable evaluation resources to assess the effectiveness of retrieval systems over sessions. The TREC Session and Tasks tracks attempted to approach this problem. The same did long time ago the TREC Interactive track. None of the above managed to produce a reusable test collection to evaluate the entire sessions of a conversation between a user and a machine. The problem remains open. 
	</p>
	<p>
	  It has become urgent for the community, and especially forums such as TREC, CLEF, NTCIR to put a focus on and provide such a setup that will put IR at the frontline in developing dynamic systems that better fit the IR needs. The Dynamic Search Lab attempts to construct such reusable test collections and metrics that will allow the development of dynamic search algorithms. The objective of the lab is threefold: 
	  <ul>
	    <li>to produce the methodology and algorithms that will lead to a dynamic test collection by simulating the users,</li>
	    <li>to understand and quantify in terms of evaluation measures what constitutes a good ranking of documents at different stages of a session, and a good overall session</li>
	    <li>to develop algorithms that can provide an optimal ranking throughout a session.</li>
	  </ul>
	</p>
      </section>
    </div>
  </div>
</section>

<section id="Lab Overview" class="wrapper style2">
  <div class="container">
    <div class="row">
      <section>
	<h3>Lab Overview</h3>
	<p>The .</p>
	
	<h3>Objectives</h3>
	<p>The <b>objective</b> of the lab is threefold:</p>
        <ol>
          <li>to produce the methodology and algorithms that will lead to a <em>dynamic test collection</em> by simulating the users</li>
          <li>to understand and quantify in terms of evaluation measures what constitutes a good ranking of documents at <em>different stages of a session</em>, and a good ranking for the overall session</li>
          <li>to develop algorithms that can provide an optimal ranking <em>throughout a user's session</em>.</li>
        </ol>
      </section>
    </div>
  </div>
</section>

<section id="Task 1" class="wrapper style1">
  <div class="container">
    <div class="row">
      <section>
	<h3>Task 1: Query Suggestion</h3>
	<h4>Objective:</h4> Given a verbose description of a task generate a sequence of queries and their corresponding rankings of the collection.
      </section>
    </div>
  </div>
</section>

<section id="Task 2" class="wrapper style1">
  <div class="container">
    <div class="row">
      <section>
	<h3>Task 2: Results Composition</h3>
		<h4>Objective:</h4> Given the ranking in Task 1 merge them in a single composite ranking.
      </section>
    </div>
  </div>
</section>


<section id="Important Dates" class="wrapper style2">            
  <div class="container">
    <div class="row">
      <h3>Important Dates</h3>
	<br>
      	<section>
      	<p>The overall schedule for the labs and the CEUR-WS Lab Working Notes is as follows:</p>
      	<p>
      	<ul>
	<li>Registration closes: 27 April 2018</li>
	<li>End of Evaluation Cycle: 11 May 2018</li>
	<li>Submission of Participant Papers [CEUR-WS]: 31 May 2018</li>
	<li>Review process of participant papers: 31 May – 15 June 2018</li>
	<li>Submission of Condensed Lab Overviews [LNCS]: 8 June 2018</li>
	<li>Notification of Acceptance Participant Papers [CEUR-WS]: 15 June 2018</li>
	<li>Notification of Acceptance Condensed Lab Overviews [LNCS]: 15 June 2018</li>
	<li>Camera Ready Copy of Condensed Lab Overviews [LNCS]: 22 June 2018</li>
	<li>Camera Ready Copy of Participant Papers and Extended Lab Overviews [CEUR-WS]: 29 June 2018</li>
	<li>CEUR-WS Working Notes Preview for Checking by Authors and Lab Organizers: 18-24 July 2018</li>
      </ul>		  
      </p>
      </section>
      <section>
      <p>The schedule for the conference and for LNCS Publication:</p>
      <p>
      <ul>
	<li>Submission of Long Papers/Best of Labs Papers: 7 May 2018</li>
	<li>Submission of Short Papers: 14 May 2018</li>
	<li>Notification of Acceptance Long/Short Papers: 8 June 2018</li>
	<li>Camera Ready Long/Short Papers: 22 June 2018</li>
      </ul>
      </p>
      </section>
    </div>
  </div>
</section>
