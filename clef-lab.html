---
layout: clef-default
---

<!-- Banner -->
<section id="banner">
  <div class="inner">
    <h2>CLEF 2018 Lab</h2>
    <p>Dynamic Search for Complex Tasks</p>
  </div>
</section>

<!-- Main -->

<section id="Timeline" class="wrapper style1">            
  <div class="container">
      <h3>Timeline</h3>
      <ul>
	      <li>Registration deadline: 27 April 2018</li>
	      <li>Training data: 15 April 2018</li>
	      <li>Testing data: 5 May 2018</li>
	      <li>Submission deadline: 11 May 2018</li>
	    </ul>
    </div>
</section>

<section id="Background" class="wrapper style2">
  <div class="container">
    <section>
      <h3>Background</h3>
      <p>Information Retrieval (IR) research has traditionally focused on serving the best results for a single query — so-called ad hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. A key challenge in the study of this interaction is the creation of suitable evaluation resources to assess the effectiveness of retrieval systems over sessions. The TREC Interactive, Session and Tasks tracks attempted to approach this problem, without managing to produce a reusable test collection to evaluate the entire sessions of a conversation between a user and a machine. The problem remains open.</p>
			<p>It has become urgent for the community, and especially forums such as TREC, CLEF, NTCIR to put a focus on and provide such a setup that will put IR at the frontline in developing dynamic systems that better fit the IR needs. The Dynamic Search Lab attempts to construct such reusable test collections and metrics that will allow the development of dynamic search algorithms. The objective of the lab is threefold: 
	 			<ul>
	   			<li>to produce the methodology and algorithms that will lead to a dynamic test collection by simulating the users,</li>
	   			<li>to understand and quantify in terms of evaluation measures what constitutes a good ranking of documents at different stages of a session, and a good overall session</li>
	   			<li>to develop algorithms that can provide an optimal ranking throughout a session.</li>
	 			</ul>
			</p>
    </section>
  </div>
</section>

<section id="Lab Overview" class="wrapper style1">
  <div class="container">
		<h3>Lab Overview</h3>
		<p>We view the problem of dynamic search as the development of two agents, a <i>question agent</i> and an <i>answer agent</i>. The two agents interact with each other towards fulfilling a user's information need.
		<ul>
			<li> <strong>A-agent:</strong> The TREC Session and Dynamic Domain tracks considered the development of advanced answer agents, retrieval systems that account both for session contextual information and the dynamic interaction with the Q-Agent when ranking documents. The 2018 DynSe Lab will consider as an answer agent a traditional ad-hoc retrieval system.</li>
			<li> <strong>Q-agent:</strong> The 2018 DynSe Lab will focus on the development of effective question agents. These agents can be viewed either as interactive query recommendation systems or as user simulators.</li>
		</ul>
		</p>
		<p> The tasks of the Lab can be viewed in the figure below: the development of a Q-agent that produces queries to be submitted to a given retrieval system, and the development of a system that composes the results of the multiple rounds of interactions between the two agents into a single ranking.</p>
	<img src="DynSe2018.png">
	</div>
</section>

<section id="Collection" class="wrapper style2">
  <div class="container">
		<h3>Collection and Retrieval System</h4>
		<p>The collection that will be used in the 2018 DynSe Lab will be the New York Times corpus <a href="https://catalog.ldc.upenn.edu/ldc2008t19">(https://catalog.ldc.upenn.edu/ldc2008t19)</a>. The New York Times dataset consists of articles published in New York Times from January 1, 1987 to June 19, 2007 with metadata provided by the New York Times Newsroom, the New York Times Indexing Service and the online production staff at nytimes.com. Most articles are manually summarized and tagged by professional staffs. The original form of this dataset is in News Industry Text Format (NITF). This dataset can aid the research in Document Categorization, Information Retrieval, Entity Extraction and etc.  The corpus has been indexed by Indri and a Query Language Model with Dirichlet Smoothing has been implemented on the top of the Indri index. Participants will be provided with a RESTful API to query the index (https://bitbucket.org/cvangysel/pyndri-flask)</p>
		<p>The RESTful API will be provided by April 15, 2018</p>
  </div>
</section>
	
	<section id="Topics" class="wrapper style1">
  <div class="container">
		<h3>Topics</h4>
		<p>The topics have been developed by the NIST assessors. A topic (which is like a query) contains a few words. It is the main search target for one complete run of dynamic search. Each topic contains multiple subtopics, each of which addresses one aspect of the topic. The NIST assessors have tried (very hard to) produce a complete set of subtopics for each topic, and so we will treat them as the complete set and use them in the interactions and evaluation</p>
		<p>Ten (10) training topics along with their subtopics will be released on April 15, 2018.</p>
		<p>Fifty (50) test topics will be released on May 5, 2018. Subtopics will not be released.</p>
  </div>
</section>
	
<section id="Task 1" class="wrapper style2">
  <div class="container">
      <section>
				<h3>Task 1: Query Suggestion</h3>
				<h4>Objective:</h4> Given a verbose description of a task (topic) generate a sequence of queries and their corresponding rankings of the collection.
				<h4>Submission Guidelines:</h4> Each Q-Agent is allowed to go over 10 rounds of query suggestions. At each round one query is submitted to the A-Agent, and the top 10 results are collected. At the end of round 10, 100 search results will have been collected.
				<h4>Submission Format:</h4> The lab will use TREC-style submissions. In TREC, a "run" is the output of a search system over ALL topics. Run Format:
				<table border="0">
  				<tr>
    			<th>TOPIC</th>
    			<th>QUESTION</th>
					<th>DOCNO</th>
    			<th>RANK</th>
					<th>SCORE</th>
    			<th>RUN</th>
  				</tr>
				<ul>
				<li>TOPIC is the topic id and can be found in the released topics</li>
				<li>QUESTION is the suggested query of this round. The question should be included into quotes, e.g. "london hotels". Each suggested query should be repeated over a maximum of 10 rows (i.e. for each one of the top 10 results obtained by submitting this query to the A-Agent.</li>
				<li>DOCNO is the document number in the corpus</li>
				<li>RANK is the rank of the document returned for this given round (in increasing order)</li>
				<li>SCORE is the score of the ranking/classification algorith</li>
				<li>RUN is an identifier/name for the system producing the run</li>
				</ul>
				<h4>Evaluation:</h4> The measurements of runs are Cube Test, sDCG and Expected Utility; other diagnostic measures such as precision and recall may also be reported. 

				<p> Cube Test is a search effectiveness measurement evaluating the speed of gaining relevant information (could be documents or passages) in a dynamic search process. It measures the amount of relevant information a system could gather and the time needed in the entire search process. The higher the Cube Test score, the better the IR system.</p>

				<p> sDCG extends the classic DCG to a search session which consists of multiple iterations. The relevance scores of results that are ranked lower or returned in later iterations get more discounts. The discounted cumulative relevance score is the final results of this metric.</p>

				<p> Expected Utility scores different runs by measuring the relevant information a system found and the length of documents. The relevance scores of documents are discounted based on ranking order and novelty. The document length is discounted only based on ranking position. The difference between the cumulative relevance score and the aggregated document length is the final score of each run.</p>
      </section>
  </div>
</section>

<section id="Task 2" class="wrapper style2">
  <div class="container">
    <div class="row">
      <section>
				<h3>Task 2: Results Composition</h3>
				<h4>Objective:</h4> Given the ranking in Task 1 merge them in a single composite ranking.
				<h4>Submission Guidelines:</h4> At the end of round 10, 100 search results will have been collected. These 100 results coming from 10 queries should be re-ranked in a single optimal ranking.
				<h4>Submission Format:</h4> The lab will use TREC-style submissions. In TREC, a "run" is the output of a search system over ALL topics. Run Format:
				<table border="0">
  				<tr>
    			<th>TOPIC</th>
    			<th>DUMMY</th>
					<th>DOCNO</th>
    			<th>RANK</th>
					<th>SCORE</th>
    			<th>RUN</th>
  				</tr>
					<ul>
				<li>TOPIC is the topic id and can be found in the released topics</li>
				<li>DUMMY is a dummy column to be filled in with 0.</li>
				<li>DOCNO is the document number in the corpus</li>
				<li>RANK is the rank of the document returned for this given round (in increasing order)</li>
				<li>SCORE is the score of the ranking/classification algorith</li>
				<li>RUN is an identifier/name for the system producing the run</li>
				</ul>
				<h4>Evaluation:</h4> The nDCG@100 will be the main evaluation measure; other evaluation measures may be reported.
      </section>
    </div>
  </div>
</section>


<section id="Important Dates" class="wrapper style1">            
  <div class="container">
		<h3>Important Dates</h3>
			<p>The overall schedule for the labs and the CEUR-WS Lab Working Notes is as follows:</p>
      	<ul>
					<li>Registration closes: 27 April 2018</li>
					<li>End of Evaluation Cycle: 11 May 2018</li>
					<li>Submission of Participant Papers [CEUR-WS]: 31 May 2018</li>
					<li>Review process of participant papers: 31 May – 15 June 2018</li>
					<li>Submission of Condensed Lab Overviews [LNCS]: 8 June 2018</li>
					<li>Notification of Acceptance Participant Papers [CEUR-WS]: 15 June 2018</li>
					<li>Notification of Acceptance Condensed Lab Overviews [LNCS]: 15 June 2018</li>
					<li>Camera Ready Copy of Condensed Lab Overviews [LNCS]: 22 June 2018</li>
					<li>Camera Ready Copy of Participant Papers and Extended Lab Overviews [CEUR-WS]: 29 June 2018</li>
					<li>CEUR-WS Working Notes Preview for Checking by Authors and Lab Organizers: 18-24 July 2018</li>
      	</ul>		  
      </p>
      <p>The schedule for the conference and for LNCS Publication:</p>
      <ul>
				<li>Submission of Long Papers/Best of Labs Papers: 7 May 2018</li>
				<li>Submission of Short Papers: 14 May 2018</li>
				<li>Notification of Acceptance Long/Short Papers: 8 June 2018</li>
				<li>Camera Ready Long/Short Papers: 22 June 2018</li>
      </ul>
      </p>
    </section>
  </div>
</section>
