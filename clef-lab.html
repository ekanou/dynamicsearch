---
layout: clef-default
---

<!-- Main -->
<section id="main" class="wrapper style1">

        <!-- Banner -->
        <section id="banner">
            <div class="inner">
                <h2>CLEF 2017 Lab</h2>
                <p>Dynamic Search for Complex Tasks</p>
            </div>
        </section>

        <div class="container">
            <div class="row">
                <section id="Lab Overview">
                    <h2>Lab Overview</h2>
                    <p>The <strong>objective</strong> of the lab is threefold:</p>
                    <ol>
                        <li>to produce the methodology and algorithms that will lead to a <em>dynamic test collection</em> by simulating the users</li>
                        <li>to understand and quantify in terms of evaluation measures what constitutes a good ranking of documents at <em>different stages of a session</em>, and a good overall session</li>
                        <li>to lead to algorithms that can provide an optimal ranking <em>throughout a session</em>.</li>
                    </ol>
                </section>
            </div>
        </div>
        
        <div class="container">
            <div class="row">
                <section id="Proposed Tasks">
                    <h3>Proposed Tasks</h3>
                        <ol>
                            <dt>Task 1:</dt>
                            <dd>
                            Construction of dynamic collections (simulations). The first task will evaluate simulations of user interactions. Simulations can be evaluated against static user logs, or actual users. This is a topic of discussion during the proposed workshop.
                            </dd>
                            <dt>Task 2:</dt>
                            <dd>Session search evaluation. The second task will evaluate proposed metrics both for overall session evaluation, and for step足by足step session evaluation. Metrics should be applied on a reusable test collection, however they can be (meta足) evaluated against an interactive experiment with actual users.
                            </dd>
                            <dt> Task 3:</dt>
                            <dd>Session search. The third task will evaluate retrieval effectiveness on the results of the two previous tasks. This third task can run in a follow足up year of the lab, after Task 1 and Task 2 have established the details of the evaluation framework.
                            </dd>
                        </ol>
                </section>
            </div>
        </div>
        
        <div class="container">
            <div class="row">        
                <section id="Participation">
                        <h3>Participation</h3>
                        <p>You can participate in the lab in two ways: (a) by submitting a scientific paper, or (b) by participating in a dry run of simulating a dynamic test collection.</p>
                        <div id="Submissions">
                                <ul class="tab">
                                        <li><a href="javascript:void(0)" class="tablinks" onclick="openTab(event, 'Scientific Papers')" id="defaultOpen">Scientific Papers</a></li>
                                        <li><a href="javascript:void(0)" class="tablinks" onclick="openTab(event, 'Simulation')">Simulation</a></li>
                                </ul>

                                <div id="Papers" class="tabcontent">
                                        <h3>Scientific Papers</h3>
                                        <p>We solicit the submission of two types of papers: position papers, and papers describing data collection or living lab opportunities.</p>
                                        <h4>Position Papers</h4>
                                        <h4>Data Papers</h4>
                                        <p>The workshop is open to the submission of papers describing test collections or data sets suitable for guiding the construction of dynamic test collections, tasks and evaluation metrics.</p>
                                </div>

                                <div id="Simulation" class="tabcontent">
                                        <h3>Simulation Evaluation (Dry Run)</h3>
                                        <p>TBD</p>
                                </div>
                </section>
            </div>
        </div>
        
        <div class="container">
            <div class="row">
                <section id="Important Dates">
                    <h3>Important Dates</h3>
                    <ul>
                        <li>Labs registration opens: 4 November 2016</li>
                        <li>Registration closes: 21 April 2017</li>
                        <li>Submission of Participant Papers [CEUR-WS]: 26 May 2017</li>
                        <li>Notification of Acceptance Participant Papers [CEUR-WS]: 16 June 2017</li>
                        <li>Camera Ready Copy of Participant Papers and Extended Lab Overviews [CEUR-WS] due: 3 July 2017</li>
                    </ul>
                </section>
            </div>
        </div>
</section>
