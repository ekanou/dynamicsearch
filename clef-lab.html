---
layout: clef-default
---

<!-- Banner -->
<section id="banner">
  	<div class="inner">
    		<h2>CLEF 2018 Lab</h2>
    		<p>Dynamic Search for Complex Tasks</p>
  	</div>
</section>

<!-- Main -->

<section id="Background" class="wrapper style1">
  <div class="container">
    <div class="row">
      <section>
        <h3>Background</h3>
        <p>Information Retrieval research has traditionally focused on serving the best results for a single query — so-called ad hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. A key challenge in the study of this interaction is the creation of suitable evaluation resources to assess the effectiveness of IR systems over sessions.</p>
	<p> The TREC Interactive track one of the first track’s at TREC, with the goal of investigating interactive information retrieval by examining the process as well as the results. It ran for nine years, from 1994 to 2002, and led to a variety of findings: Presentation of documents matters to users, i.e., better surrogates help but clustering does not; users do not utilize relevance feedback as we might think/hope; results from “batch” studies do not necessarily apply to real-world searchers. Further, it resulted in an early standardization of the experimental design towards evaluating interactive information retrieval systems. However, the track did not lead to a reusable test collection methodology.</p>
	<p> The High Accuracy Retrieval of Documents (HARD) track followed the Interactive track, with the primary focus on single-cycle user-system interactions. These interactions were embodied in clarification forms which could be used by retrieval algorithms to elicit feedback from assessors. The track attempted to further standardize the retrieval of interactive algorithms, however it also did not lead to a reusable collection, in the sense that new retrieval algorithms might not have received the same clarification forms as those participating in the experiment, had they interacted with the searchers.</p>
	<p> The TREC Session Track, which ran from 2010 through 2014, had always had two primary goals: (1) to test whether systems can improve their performance for a given query by using previous user interactions with a retrieval system (including previous queries, clicks on ranked results, dwell times, etc.), and (2) to evaluate system performance over an entire query session instead of a single query. However, it only produced test collections to evaluate  the first goal, given that the second one required a dynamic test collection, where users react upon the results returned by a retrieval system.</p>
	<p>In 2015 the TREC Tasks track was introduced to test whether systems upon a user’s query could understand the underlying information access task and provide documents for all possible subtasks as a way to evaluate the second goal of the Session track. However, this stripped out the task from any user interactions and the dynamics of any such collection.</p>
      </section>
    </div>
  </div>
</section>

<section id="Lab Overview" class="wrapper style2">
  <div class="container">
    <div class="row">
      <section>
	<h3>Lab Overview</h3>
	<p>The present lab attempts to bridge the batch TREC-style evaluation methodology and the Interactive Information Retrieval evaluation methodology, with the goal of evaluating system performance over an <strong>entire session</strong>, keeping the “user” in the loop, but at the same time leading to reusable test collections.</p>
	
	<h3>Objectives</h3>
	<p>The <b>objective</b> of the lab is threefold:</p>
        <ol>
          <li>to produce the methodology and algorithms that will lead to a <em>dynamic test collection</em> by simulating the users</li>
          <li>to understand and quantify in terms of evaluation measures what constitutes a good ranking of documents at <em>different stages of a session</em>, and a good ranking for the overall session</li>
          <li>to develop algorithms that can provide an optimal ranking <em>throughout a user's session</em>.</li>
        </ol>
      </section>
    </div>
  </div>
</section>

<section id="Task 1" class="wrapper style1">
  <div class="container">
    <div class="row">
      <section>
	<h3>Task 1: Query Suggestion</h3>
	<h4>Objective:</h4> Given a verbose description of a task generate a sequence of queries and their corresponding rankings of the collection.
      </section>
    </div>
  </div>
</section>

<section id="Task 2" class="wrapper style1">
  <div class="container">
    <div class="row">
      <section>
	<h3>Task 2: Results Composition</h3>
		<h4>Objective:</h4> Given the ranking in Task 1 merge them in a single composite ranking.
      </section>
    </div>
  </div>
</section>


<section id="Important Dates" class="wrapper style2">            
  <div class="container">
    <div class="row">
      <h3>Important Dates</h3>
      <p>The overall schedule for the labs and the CEUR-WS Lab Working Notes is as follows:</p>
      <p>
      <ul>
	<li>Registration opens: 8 November 2017</li>
	<li>Registration closes: 27 April 2018</li>
	<li>End of Evaluation Cycle: 11 May 2018</li>
	<li>Submission of Participant Papers [CEUR-WS]: 31 May 2018</li>
	<li>Review process of participant papers: 31 May – 15 June 2018</li>
	<li>Submission of Condensed Lab Overviews [LNCS]: 8 June 2018</li>
	<li>Notification of Acceptance Participant Papers [CEUR-WS]: 15 June 2018</li>
	<li>Notification of Acceptance Condensed Lab Overviews [LNCS]: 15 June 2018</li>
	<li>Camera Ready Copy of Condensed Lab Overviews [LNCS]: 22 June 2018</li>
	<li>Camera Ready Copy of Participant Papers and Extended Lab Overviews [CEUR-WS]: 29 June 2018</li>
	<li>CEUR-WS Working Notes Preview for Checking by Authors and Lab Organizers: 18-24 July 2018</li>
      </ul>		  
      </p>
      
      <p>The schedule for the conference and for LNCS Publication:</p>
      <p>
      <ul>
	<li>Submission of Long Papers/Best of Labs Papers: 7 May 2018</li>
	<li>Submission of Short Papers: 14 May 2018</li>
	<li>Notification of Acceptance Long/Short Papers: 8 June 2018</li>
	<li>Camera Ready Long/Short Papers: 22 June 2018</li>
      </ul>
      </p>
    </div>
  </div>
</section>
