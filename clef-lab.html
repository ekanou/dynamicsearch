---
layout: clef-default
---

<!-- Banner -->
<section id="banner">
  	<div class="inner">
    		<h2>CLEF 2017 Lab</h2>
    		<p>Dynamic Search for Complex Tasks</p>
  	</div>
</section>

<!-- Main -->
<section id="overview" class="wrapper style1">
        <div class="container">
            	<div class="row">
			<section>
            			<h3>Lab Overview</h3>
                		<p>Information Retrieval research has traditionally focused on serving the best results for a single query — so-called ad hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. A key challenge in the study of this interaction is the creation of suitable evaluation resources to assess the effectiveness of IR systems over sessions. The TREC Session Track, which ran from 2010 through to 2014, had always had two primary goals: (1) to test whether systems can improve their performance for a given query by using previous user interactions with a retrieval system (including previous queries, clicks on ranked results, dwell times, etc.), and (2) to evaluate system performance over an entire query session instead of a single query. However, it only produced test collections to evaluate  the first goal, given that the second one required a dynamic test collection, where users react upon the results returned by a retrieval system. In 2015 the TREC Tasks track was introduced to test whether systems upon a user’s query could understand the underlying information access task and provide documents for all possible subtasks as a way to evaluate the second goal of the Session track. However, this stripped out the task from any user interactions and the dynamics of any such collection. The present lab attempts to bridge the two evaluation exercises, with the goal of evaluating system performance over an entire session, keeping the “user” in the loop.</p>
			</section>
		</div>
	</div>
</section>

<section id="objectives" class="wrapper style2">
        <div class="container">
            	<div class="row">
			<section>
				<h3>Objectives</h3>
		    		<p>The <b>objective</b> of the lab is threefold:</p>
                    		<ol>
                        	<li>to produce the methodology and algorithms that will lead to a <em>dynamic test collection</em> by simulating the users</li>
                        	<li>to understand and quantify in terms of evaluation measures what constitutes a good ranking of documents at <em>different stages of a session</em>, and a good ranking for the overall session</li>
                        	<li>to develop algorithms that can provide an optimal ranking <em>throughout a user's session</em>.</li>
                    		</ol>
			</section>
		</div>
	</div>
</section>

<section id="participate" class="wrapper style1">
     	<div class="container">
            	<div class="row">
			<section>
		 	<h3>Participation</h3>
                        <p>This is the first year that this workshop runs and there are two possible ways to participate:</p>
                        <div id="Submissions">
                                <ul class="tab">
                                        <li><a href="javascript:void(0)" class="tablinks" onclick="openTab(event, 'Papers')" id="defaultOpen">Scientific Papers</a></li>
                                        <li><a href="javascript:void(0)" class="tablinks" onclick="openTab(event, 'Pilot Task')">Pilot Task</a></li>
                                </ul>

                                <div id="Papers" class="tabcontent">
                                        <p>We solicit the submission of two types of papers: position papers, and papers describing data collection or living lab opportunities.</p>
                                        <p><em>Position papers</em> focusing on evaluation methodologies for assessing the quality of search algorithms with the user in the loop, under two constraints: (1) any evaluation framework proposed should allow the (statistical) reproducibility of results, and (2) it should lead to a reusable benchmark collection.</p>
                                        <p><em>Data Papers</em> describing test collections or data sets suitable for guiding the construction of dynamic test collections, tasks and evaluation metrics.</p>
                                        <p> </div>

                                <div id="Pilot Task" class="tabcontent">
                                  <p>Submit a run that simulates query reformulations</p>
                                  <p> Dataset: TREC 2014 Session Track
                                  <dl>
                                    <dt> Input: </dt>
                                    <dt>
                                    <ol>
                                    <li>Task description</li>
                                    <li>The first query of a user in a session</li>
                                    <li>The ranking for the first query, the clicks, the dwell times, and the relevance labels</li>
                                    <li>The set of queries for the same  but from all of the users that tried to complete this task, throughout their sessions</li>
                                    </ol>
                                    </dt>
                                    <dt>Output: Ranking of next user's query.</dt>
                                    <dt>Evaluation Measure: # of user queries correctly predicted</dt>
                                  </dl>
                                </div>
                        </div>
			</section>
            	</div>
	</div>
</section>
	
        <!--
        <div class="container">
            <div class="row">
                <section id="Proposed Tasks">
                    <h3>Proposed Tasks</h3>
                        <ol>
                            <dt>Task 1:</dt>
                            <dd>
                            Construction of dynamic collections (simulations). The first task will evaluate simulations of user interactions. Simulations can be evaluated against static user logs, or actual users. This is a topic of discussion during the proposed workshop.
                            </dd>
                            <dt>Task 2:</dt>
                            <dd>Session search evaluation. The second task will evaluate proposed metrics both for overall session evaluation, and for step­by­step session evaluation. Metrics should be applied on a reusable test collection, however they can be (meta­) evaluated against an interactive experiment with actual users.
                            </dd>
                            <dt> Task 3:</dt>
                            <dd>Session search. The third task will evaluate retrieval effectiveness on the results of the two previous tasks. This third task can run in a follow­up year of the lab, after Task 1 and Task 2 have established the details of the evaluation framework.
                            </dd>
                        </ol>
                </section>
            </div>
        </div>
        -->
        
<section id="dates" class="wrapper style2">            
        <div class="container">
            	<div class="row">
                    	<h3>Important Dates</h3>
                    	<ul>
                        <li>Labs registration opens: 4 November 2016</li>
                        <li>Registration closes: 21 April 2017</li>
                        <li>Submission of Participant Papers [CEUR-WS]: 26 May 2017</li>
                        <li>Notification of Acceptance Participant Papers [CEUR-WS]: 16 June 2017</li>
                        <li>Camera Ready Copy of Participant Papers and Extended Lab Overviews [CEUR-WS] due: 3 July 2017</li>
                    	</ul>
            	</div>
        </div>
</section>
