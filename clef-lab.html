---
layout: clef-default
---

<!-- Banner -->
<section id="banner">
  <div class="inner">
    <h2>CLEF 2018 Lab</h2>
    <p>Dynamic Search for Complex Tasks</p>
  </div>
</section>

<!-- Main -->

<section id="Timeline" class="wrapper style1">            
  <div class="container">
      <h3>Timeline</h3>
      <ul>
	      <li>Registration deadline: 27 April 2018</li>
	      <li>Training data: 15 April 2018</li>
	      <li>Testing data: 5 May 2018</li>
	      <li>Submission deadline: 11 May 2018</li>
	    </ul>
    </div>
</section>

<section id="Background" class="wrapper style2">
  <div class="container">
    <section>
      <h3>Background</h3>
      <p>Information Retrieval (IR) research has traditionally focused on serving the best results for a single query — so-called ad hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. A key challenge in the study of this interaction is the creation of suitable evaluation resources to assess the effectiveness of retrieval systems over sessions. The TREC Session and Tasks tracks attempted to approach this problem. The same did long time ago the TREC Interactive track. None of the above managed to produce a reusable test collection to evaluate the entire sessions of a conversation between a user and a machine. The problem remains open.</p>
			<p>It has become urgent for the community, and especially forums such as TREC, CLEF, NTCIR to put a focus on and provide such a setup that will put IR at the frontline in developing dynamic systems that better fit the IR needs. The Dynamic Search Lab attempts to construct such reusable test collections and metrics that will allow the development of dynamic search algorithms. The objective of the lab is threefold: 
	 			<ul>
	   			<li>to produce the methodology and algorithms that will lead to a dynamic test collection by simulating the users,</li>
	   			<li>to understand and quantify in terms of evaluation measures what constitutes a good ranking of documents at different stages of a session, and a good overall session</li>
	   			<li>to develop algorithms that can provide an optimal ranking throughout a session.</li>
	 			</ul>
			</p>
    </section>
  </div>
</section>

<section id="Lab Overview" class="wrapper style1">
  <div class="container">
		<h3>Lab Overview</h3>
		<p>We view the problem of dynamic search as the development of two agents, a <i>question agent</i> and an <i>answer agent</i>. The two agents interact with each other towards fulfilling a user's information need.</p>
		<ul>
			<li> <strong>A-Agent:</strong> The 2018 DynSe Lab will consider as an answer agent a traditional ad-hoc retrieval system. The TREC Session and Dynamic Domain tracks, considered the development of advanced answer agents.</li>
			<li> <strong>Q-Agent:</strong> The 2018 DynSe Lab will consider the development of effective question agents. These agents can be viewed either as interactive query recommendation systems or as user simulators.</li>
		</ul>
  </div>
</section>

<section id="Task 1" class="wrapper style2">
  <div class="container">
    <div class="row">
      <section>
	<h3>Task 1: Query Suggestion</h3>
	<h4>Objective:</h4> Given a verbose description of a task generate a sequence of queries and their corresponding rankings of the collection.
      </section>
    </div>
  </div>
</section>

<section id="Task 2" class="wrapper style2">
  <div class="container">
    <div class="row">
      <section>
	<h3>Task 2: Results Composition</h3>
		<h4>Objective:</h4> Given the ranking in Task 1 merge them in a single composite ranking.
      </section>
    </div>
  </div>
</section>


<section id="Important Dates" class="wrapper style1">            
  <div class="container">
		<h3>Important Dates</h3>
			<p>The overall schedule for the labs and the CEUR-WS Lab Working Notes is as follows:</p>
      	<ul>
					<li>Registration closes: 27 April 2018</li>
					<li>End of Evaluation Cycle: 11 May 2018</li>
					<li>Submission of Participant Papers [CEUR-WS]: 31 May 2018</li>
					<li>Review process of participant papers: 31 May – 15 June 2018</li>
					<li>Submission of Condensed Lab Overviews [LNCS]: 8 June 2018</li>
					<li>Notification of Acceptance Participant Papers [CEUR-WS]: 15 June 2018</li>
					<li>Notification of Acceptance Condensed Lab Overviews [LNCS]: 15 June 2018</li>
					<li>Camera Ready Copy of Condensed Lab Overviews [LNCS]: 22 June 2018</li>
					<li>Camera Ready Copy of Participant Papers and Extended Lab Overviews [CEUR-WS]: 29 June 2018</li>
					<li>CEUR-WS Working Notes Preview for Checking by Authors and Lab Organizers: 18-24 July 2018</li>
      	</ul>		  
      </p>
      <p>The schedule for the conference and for LNCS Publication:</p>
      <ul>
				<li>Submission of Long Papers/Best of Labs Papers: 7 May 2018</li>
				<li>Submission of Short Papers: 14 May 2018</li>
				<li>Notification of Acceptance Long/Short Papers: 8 June 2018</li>
				<li>Camera Ready Long/Short Papers: 22 June 2018</li>
      </ul>
      </p>
    </section>
  </div>
</section>
