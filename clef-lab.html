---
layout: clef-default
---

<!-- Banner -->
<section id="banner">
  	<div class="inner">
    		<h2>CLEF 2018 Lab</h2>
    		<p>Dynamic Search for Complex Tasks</p>
  	</div>
</section>

<!-- Main -->

<section id="Lab Overview" class="wrapper style1">
  <div class="container">
    <div class="row">
      <section>
        <h3>Background</h3>
        <p>Information Retrieval research has traditionally focused on serving the best results for a single query — so-called ad hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. A key challenge in the study of this interaction is the creation of suitable evaluation resources to assess the effectiveness of IR systems over sessions.</p>
	<p> The TREC Interactive track one of the first track’s at TREC, with the goal of investigating interactive information retrieval by examining the process as well as the results. It ran for nine years, from 1994 to 2002, and led to a variety of findings: Presentation of documents matters to users, i.e., better surrogates help but clustering does not; users do not utilize relevance feedback as we might think/hope; results from “batch” studies do not necessarily apply to real-world searchers. Further, it resulted in an early standardization of the experimental design towards evaluating interactive information retrieval systems. However, the track did not lead to a reusable test collection methodology.</p>
	<p> The High Accuracy Retrieval of Documents (HARD) track followed the Interactive track, with the primary focus on single-cycle user-system interactions. These interactions were embodied in clarification forms which could be used by retrieval algorithms to elicit feedback from assessors. The track attempted to further standardize the retrieval of interactive algorithms, however it also did not lead to a reusable collection, in the sense that new retrieval algorithms might not have received the same clarification forms as those participating in the experiment, had they interacted with the searchers.</p>
	<p> The TREC Session Track, which ran from 2010 through 2014, had always had two primary goals: (1) to test whether systems can improve their performance for a given query by using previous user interactions with a retrieval system (including previous queries, clicks on ranked results, dwell times, etc.), and (2) to evaluate system performance over an entire query session instead of a single query. However, it only produced test collections to evaluate  the first goal, given that the second one required a dynamic test collection, where users react upon the results returned by a retrieval system.</p>
	<p>In 2015 the TREC Tasks track was introduced to test whether systems upon a user’s query could understand the underlying information access task and provide documents for all possible subtasks as a way to evaluate the second goal of the Session track. However, this stripped out the task from any user interactions and the dynamics of any such collection.</p>
      </section>
    </div>
  </div>
</section>

<section id="Objectives" class="wrapper style2">
  <div class="container">
    <div class="row">
      <section>
	<h3>Lab Overview</h3>
	<p>The present lab attempts to bridge the batch TREC-style evaluation methodology and the Interactive Information Retrieval evaluation methodology, with the goal of evaluating system performance over an <strong>entire session</strong>, keeping the “user” in the loop, but at the same time leading to reusable test collections.</p>
	
	<h3>Objectives</h3>
	<p>The <b>objective</b> of the lab is threefold:</p>
        <ol>
          <li>to produce the methodology and algorithms that will lead to a <em>dynamic test collection</em> by simulating the users</li>
          <li>to understand and quantify in terms of evaluation measures what constitutes a good ranking of documents at <em>different stages of a session</em>, and a good ranking for the overall session</li>
          <li>to develop algorithms that can provide an optimal ranking <em>throughout a user's session</em>.</li>
        </ol>
      </section>
    </div>
  </div>
</section>

<section id="Task 1" class="wrapper style1">
  <div class="container">
    <div class="row">
      <section>
	<h3>Task 1: Query Suggestion</h3>

<--	
	<h4>Introduction</h4>
	<p>Task Completion Engines and Autonomous Search Agents are now being developed to help users in acquiring information in order to make a decision and complete a search task. At the same time such Autonomous Search Agent present a potential in being used to simulate users submitting queries, which can enable the evaluation of dynamic search algorithms.</p>
	
	<p>Such engines/agents need to work with a user to ascertain their information needs, then perform their own searches to dynamically identify relevant material, which will be useful in completing a particular task. For example, consider the task of organising a wedding. There are many different things that that need to be arranged and ordered, e.g. a venue, flowers, catering, gift list, dresses, car hire, hotels, etc. Finding relevant sites and resources requires numerous searches and filtering through many documents/sites. A search agent could help to expedite the process by finding the relevant sites to visit, while a task completion engine would provide a structured interface to help complete the process.</p>
	
	<p>In this year's Dynamic Search Task Track, the task can be interpreted in one of two ways:
          <ul>
            <li> (a) to generate a series of queries that a search agent would issue to a search engine, in order to compile a set of links useful for the user. This set might be presented to the user or used for further processing by a task completion engine.</li>
            <li> or, (b) to generate a series of query suggestions that a search engine would recommend to the user, and thus the suggested course of interaction.</li>
        </p>
	
	<p>As a starting point, we will consider how people look for information required to complete their work tasks, and use this information as a reference point for our evaluation, to determine if we can we develop search agents that can generate queries that are like those posed by people (or if we can provide suggestions that person would follow)? Thus the focus of the track will be on query generation and models of querying.</p>
      </section>
      
      <section>
	<h4> Task Overview </h4>
	<ul>
	  <li>Task Description: Starting with an initial query for a given topic, the task is to generate a series of subsequent or related queries.</li>
	  <li>Data Set: The data used will TREC ClueWeb Part B and based on the topics from the Session Track 2014.</li>
          <li>API: ClueWeb has been indexed using ElasticSearch, so you can assume that the search engine that the agent/person is interacting with is based on this API (see below for details on how to query the API).</li>
          <li>Topics: A subset of 26 topics have been selected, where there was many queries issued, numerous clicks, and relevant documents i.e. interaction data. </li>
          <li>Feedback: To help guide the process, also included is noisy judgements / click data, where in the case (a) the feedback data can be taken to be a classifier making decisions on the observed result lists (i.e. from what is returned by the queries issued), or in the case of (b) the feedback data can be take as clicks that the user has performed on the observed result list (i.e. given a query, you can assume that this is what the user clicks on, to help infer the next query).</li>
	</ul>
      </section>
      
      <section>
	<h4> Submission Guidelines</h4>
	<p>
          <ul>
            <li>For each topic, you may provide up to 5 suggested queries (topic query_no query_string) one per line. The goal is to recommend queries most like those issued by people which yield relevant information.</li>
            <li> And, a result list, which contains up to 50 results per topic (in TREC format). The 50 results should be based on a combination of the results retrieved by the 5 queries. It can be any combination, e.g. 30 results from query 1, 10 results from query 2, 10 from query 4, or 50 results from query 1, or 10 results from each query. The goal is to retrieve as many of the relevant documents as possible.</li>
            <li>For each run submitted, please indicate how your method works, and what interaction data was used in the process, and how many interactions were required i.e. how many queries generated and submitted (as a number could be generated, but only 5 selected), i.e. how much effort went into finding those queries. In the (b) case, assume interaction is linear, such that a subsequent query is based on the previous interaction. You can submit this report in the participants notebook paper.</li>
            
          </ul>
        </p>
      </section>
      
      <section>
	<h4> Evaluation</h4>
	<p>As this is a workshop/lab, we are open to different ways in which to evaluate this task. Some basic measures that we will employ are:
          <ul>
            <li>Query term overlap: how well do the query terms in the suggestions match with the terms used</li>
            <li>Query likelihood: how likely are the queries suggested given the model of relevance.</li>
            <li>Precision and Recall based measures on the set of documents retrieved.</li>
            <li>Suggest your own measure: how to measure the usefulness and value of queries is a rather open question. So how can we evaluate how good a set of queries are in relation to completing a particular task?</li>
          </ul>
        </p>
      </section>
      -->
    </div>
  </div>
</section>

<section id="Task 2" class="wrapper style2">
  <div class="container">
    <div class="row">
      <section>
	<h3>Task 2: Results Composition</h3>
      </section>
    </div>
  </div>
</section>


<section id="Important Dates" class="wrapper style1">            
  <div class="container">
    <div class="row">
      <h3>Important Dates</h3>
      The overall schedule for the labs and the CEUR-WS Lab Working Notes is as follows:<br>
      <ul>
	<li>Registration opens: 8 November 2017</li>
	<li>Registration closes: 27 April 2018</li>
	<li>End of Evaluation Cycle: 11 May 2018</li>
	<li>Submission of Participant Papers [CEUR-WS]: 31 May 2018</li>
	<li>Review process of participant papers: 31 May – 15 June 2018</li>
	<li>Submission of Condensed Lab Overviews [LNCS]: 8 June 2018</li>
	<li>Notification of Acceptance Participant Papers [CEUR-WS]: 15 June 2018</li>
	<li>Notification of Acceptance Condensed Lab Overviews [LNCS]: 15 June 2018</li>
	<li>Camera Ready Copy of Condensed Lab Overviews [LNCS]: 22 June 2018</li>
	<li>Camera Ready Copy of Participant Papers and Extended Lab Overviews [CEUR-WS]: 29 June 2018</li>
	<li>CEUR-WS Working Notes Preview for Checking by Authors and Lab Organizers: 18-24 July 2018</li>
      </ul>		  
      
      The schedule for the conference and for LNCS Publication:<br>
      <ul>
	<li>Submission of Long Papers/Best of Labs Papers: 7 May 2018</li>
	<li>Submission of Short Papers: 14 May 2018</li>
	<li>Notification of Acceptance Long/Short Papers: 8 June 2018</li>
	<li>Camera Ready Long/Short Papers: 22 June 2018</li>
      </ul>
    </div>
  </div>
</section>
